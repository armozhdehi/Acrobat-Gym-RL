{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-11 06:03:27.747152: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-11 06:03:27.755732: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-11 06:03:27.766485: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-11 06:03:27.766510: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-11 06:03:27.773863: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-11 06:03:28.567382: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/arash/.local/lib/python3.10/site-packages/pydantic/_internal/_fields.py:161: UserWarning: Field \"model_file\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/home/arash/.local/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "2024-07-11 06:03:29.884251: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-11 06:03:29.925365: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-11 06:03:29.925403: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-11 06:03:29.927574: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-11 06:03:29.927604: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-11 06:03:29.927617: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-11 06:03:30.001812: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-11 06:03:30.001863: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-11 06:03:30.001868: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2019] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-07-11 06:03:30.001887: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-11 06:03:30.001903: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5563 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add the 'nets' folder to the system path to import the necessary modules\n",
    "sys.path.append('./nets')\n",
    "\n",
    "from DDPGAgent import DDPGAgent, DDPGAgentConfig\n",
    "from Memory import Memory, MemoryConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_frames_as_gif(frames, path='./', filename='gym_animation.gif'):\n",
    "    import imageio\n",
    "    imageio.mimsave(os.path.join(path, filename), frames, fps=30)\n",
    "\n",
    "def plot_learning_curve(scores, filename):\n",
    "    x = [i+1 for i in range(len(scores))]\n",
    "    plt.plot(x, scores)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Score')\n",
    "    plt.savefig(filename)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(agent, env, num_games, scores_plot_file):\n",
    "    scores = []\n",
    "    best_avg_score = -np.inf\n",
    "    \n",
    "    for t in range(num_games):\n",
    "        state, _ = env.reset()\n",
    "        terminated, truncated = False, False\n",
    "        score = 0\n",
    "        step = 0\n",
    "        agent.noise.reset()\n",
    "        while not (terminated or truncated):\n",
    "            step += 1\n",
    "            action = agent.act(state)\n",
    "            action = np.argmax(action)  # Convert continuous action to discrete action\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            terminal = terminated or truncated\n",
    "            agent.memory.store(state, action, reward, next_state, terminal)\n",
    "            agent.learn()\n",
    "            score += reward \n",
    "            state = next_state\n",
    "        scores.append(score)\n",
    "        \n",
    "        avg_score = np.mean(scores[-100:])\n",
    "        print(f\"game {t}, steps {step}, score {score:.2f}, avg_score {avg_score:.2f}\")\n",
    "        if avg_score > best_avg_score:\n",
    "            agent.save_models()\n",
    "            best_avg_score = avg_score\n",
    "\n",
    "    plot_learning_curve(scores, scores_plot_file)\n",
    "\n",
    "def test_agent(agent, env, final_landing_file):\n",
    "    agent.load_models()\n",
    "    \n",
    "    frames = []\n",
    "    terminated, truncated = False, False\n",
    "    score = 0\n",
    "    state, _ = env.reset()\n",
    "    step = 0\n",
    "    agent.noise.reset()\n",
    "    while not (terminated or truncated):\n",
    "        step += 1\n",
    "        action = agent.act(state)\n",
    "        action = np.argmax(action)  # Convert continuous action to discrete action\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        terminal = terminated or truncated\n",
    "        score += reward \n",
    "        state = next_state\n",
    "        frames.append(env.render())\n",
    "\n",
    "    print(f\"score {score:.2f}\")      \n",
    "    save_frames_as_gif(frames, final_landing_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arash/.local/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "states: (64, 6), actions: (64, 3, 3), rewards: (64,), next_states: (64, 6), dones: (64,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arash/.local/lib/python3.10/site-packages/keras/src/layers/layer.py:1331: UserWarning: Layer 'critic_net' looks like it has unbuilt state, but Keras is not able to trace the layer `call()` in order to build it automatically. Possible causes:\n",
      "1. The `call()` method of your layer may be crashing. Try to `__call__()` the layer eagerly on some test input first to see if it works. E.g. `x = np.random.random((3, 4)); y = layer(x)`\n",
      "2. If the `call()` method is correct, then you may need to implement the `def build(self, input_shape)` method on your layer. It should create all variables used by the layer (e.g. by calling `layer.build()` on all its children layers).\n",
      "Exception encountered: ''Dimensions must be equal, but are 64 and 3 for '{{node Add}} = AddV2[T=DT_FLOAT](layer_normalization_5_1/add_2, dense_8_1/Add)' with input shapes: [64,300], [64,3,300].''\n",
      "  warnings.warn(\n",
      "/home/arash/.local/lib/python3.10/site-packages/keras/src/layers/layer.py:372: UserWarning: `build()` was called on layer 'critic_net', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "2024-07-11 06:03:31.041692: W tensorflow/core/framework/op_kernel.cc:1827] INVALID_ARGUMENT: required broadcastable shapes\n",
      "2024-07-11 06:03:31.041729: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: INVALID_ARGUMENT: required broadcastable shapes\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Exception encountered when calling CriticNet.call().\n\n\u001b[1m{{function_node __wrapped__AddV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} required broadcastable shapes [Op:AddV2] name: \u001b[0m\n\nArguments received by CriticNet.call():\n  • inputs=['tf.Tensor(shape=(64, 6), dtype=float32)', 'tf.Tensor(shape=(64, 3, 3), dtype=float32)']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 51\u001b[0m\n\u001b[1;32m     48\u001b[0m         test_agent(agent, env, final_landing_file)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Run the main function\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Change to \"test\" for testing\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 46\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(mode)\u001b[0m\n\u001b[1;32m     43\u001b[0m agent \u001b[38;5;241m=\u001b[39m DDPGAgent(agent_config)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 46\u001b[0m     \u001b[43mtrain_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_games\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscores_plot_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     48\u001b[0m     test_agent(agent, env, final_landing_file)\n",
      "Cell \u001b[0;32mIn[3], line 18\u001b[0m, in \u001b[0;36mtrain_agent\u001b[0;34m(agent, env, num_games, scores_plot_file)\u001b[0m\n\u001b[1;32m     16\u001b[0m terminal \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n\u001b[1;32m     17\u001b[0m agent\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39mstore(state, action, reward, next_state, terminal)\n\u001b[0;32m---> 18\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward \n\u001b[1;32m     20\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n",
      "File \u001b[0;32m~/Acrobat-Gym-RL/DDPG/./nets/DDPGAgent.py:187\u001b[0m, in \u001b[0;36mDDPGAgent.learn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    185\u001b[0m target_actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_actor(next_states)\n\u001b[1;32m    186\u001b[0m target_Q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_critic([next_states, target_actions])\n\u001b[0;32m--> 187\u001b[0m online_Q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43monline_critic\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m target_Q_values \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mwhere(dones, tf\u001b[38;5;241m.\u001b[39mzeros_like(target_Q_values), target_Q_values)\n\u001b[1;32m    190\u001b[0m target_Q_values \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39msqueeze(target_Q_values)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/Acrobat-Gym-RL/DDPG/./nets/CriticNet.py:71\u001b[0m, in \u001b[0;36mCriticNet.call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     69\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln2(s)\n\u001b[1;32m     70\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_layer(action)\n\u001b[0;32m---> 71\u001b[0m s_a \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mrelu(\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     72\u001b[0m Q_s_a \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mQ_value(s_a)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Q_s_a\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Exception encountered when calling CriticNet.call().\n\n\u001b[1m{{function_node __wrapped__AddV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} required broadcastable shapes [Op:AddV2] name: \u001b[0m\n\nArguments received by CriticNet.call():\n  • inputs=['tf.Tensor(shape=(64, 6), dtype=float32)', 'tf.Tensor(shape=(64, 3, 3), dtype=float32)']"
     ]
    }
   ],
   "source": [
    "def main(mode=\"train\"):\n",
    "    env_name = \"Acrobot-v1\"\n",
    "    env = gym.make(env_name)\n",
    "    num_games = 1000\n",
    "    a_lr = 0.0001\n",
    "    c_lr = 0.001\n",
    "    gamma = 0.99\n",
    "    tau = 0.001\n",
    "    input_size = env.observation_space.shape[0]\n",
    "    fcl1_size = 400\n",
    "    fcl2_size = 300\n",
    "    actions_num = env.action_space.n  # Note the change to .n for discrete actions\n",
    "    memory_size = 1000000\n",
    "    batch_size = 64\n",
    "    \n",
    "    file_name = f\"DDPG_{env_name}_{a_lr}_{c_lr}_{num_games}\"\n",
    "    scores_plot_file = f\"./plots/{file_name}.png\"\n",
    "    final_landing_file = f\"./plots/{file_name}.gif\"\n",
    "    actor_file_name = f\"Actor_DDPG_{env_name}_{a_lr}_{num_games}\"\n",
    "    critic_file_name = f\"Critic_DDPG_{env_name}_{c_lr}_{num_games}\"\n",
    "    oa_mf = f\"./models/Online_{actor_file_name}.h5\"\n",
    "    oc_mf = f\"./models/Online_{critic_file_name}.h5\"\n",
    "    ta_mf = f\"./models/Target_{actor_file_name}.h5\"\n",
    "    tc_mf = f\"./models/Target_{critic_file_name}.h5\"\n",
    "    \n",
    "    agent_config = DDPGAgentConfig(\n",
    "        actor_lr=a_lr,\n",
    "        critic_lr=c_lr,\n",
    "        gamma=gamma,\n",
    "        tau=tau,\n",
    "        input_dim=input_size,\n",
    "        fc1_units=fcl1_size,\n",
    "        fc2_units=fcl2_size,\n",
    "        action_dim=actions_num,\n",
    "        memory_size=memory_size,\n",
    "        batch_size=batch_size,\n",
    "        actor_model_file=oa_mf,\n",
    "        critic_model_file=oc_mf,\n",
    "        target_actor_model_file=ta_mf,\n",
    "        target_critic_model_file=tc_mf\n",
    "    )\n",
    "    \n",
    "    agent = DDPGAgent(agent_config)\n",
    "    \n",
    "    if mode == \"train\":\n",
    "        train_agent(agent, env, num_games, scores_plot_file)\n",
    "    elif mode == \"test\":\n",
    "        test_agent(agent, env, final_landing_file)\n",
    "\n",
    "# Run the main function\n",
    "main(mode=\"train\")  # Change to \"test\" for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-11 06:07:09.068508: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-11 06:07:09.075921: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-11 06:07:09.087823: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-11 06:07:09.087850: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-11 06:07:09.094876: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-11 06:07:09.504572: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/arash/.local/lib/python3.10/site-packages/pydantic/_internal/_fields.py:161: UserWarning: Field \"model_file\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/home/arash/.local/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "2024-07-11 06:07:10.101836: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-11 06:07:10.118946: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-11 06:07:10.118981: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-11 06:07:10.121046: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-11 06:07:10.121070: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-11 06:07:10.121080: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-11 06:07:10.200215: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-11 06:07:10.200259: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-11 06:07:10.200263: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2019] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-07-11 06:07:10.200281: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-11 06:07:10.200297: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5563 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 127\u001b[0m\n\u001b[1;32m    124\u001b[0m         test_agent(agent, env, final_landing_file)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# Run the main function\u001b[39;00m\n\u001b[0;32m--> 127\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Change to \"test\" for testing\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 122\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(mode)\u001b[0m\n\u001b[1;32m    119\u001b[0m agent \u001b[38;5;241m=\u001b[39m DDPGAgent(agent_config)\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 122\u001b[0m     \u001b[43mtrain_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_games\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscores_plot_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    124\u001b[0m     test_agent(agent, env, final_landing_file)\n",
      "Cell \u001b[0;32mIn[1], line 40\u001b[0m, in \u001b[0;36mtrain_agent\u001b[0;34m(agent, env, num_games, scores_plot_file)\u001b[0m\n\u001b[1;32m     38\u001b[0m step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     39\u001b[0m action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mact(state)\n\u001b[0;32m---> 40\u001b[0m next_state, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m terminal \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n\u001b[1;32m     42\u001b[0m agent\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39mstore(state, action, reward, next_state, terminal)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gym/wrappers/time_limit.py:50\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m     40\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \n\u001b[1;32m     42\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gym/wrappers/order_enforcing.py:37\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gym/wrappers/env_checker.py:37\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchecked_step \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchecked_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43menv_step_passive_checker\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:214\u001b[0m, in \u001b[0;36menv_step_passive_checker\u001b[0;34m(env, action)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"A passive check for the environment step, investigating the returning data then returning the data unchanged.\"\"\"\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;66;03m# We don't check the action as for some environments then out-of-bounds values can be given\u001b[39;00m\n\u001b[0;32m--> 214\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    216\u001b[0m     result, \u001b[38;5;28mtuple\u001b[39m\n\u001b[1;32m    217\u001b[0m ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpects step result to be a tuple, actual type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(result)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(result) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gym/envs/classic_control/acrobot.py:199\u001b[0m, in \u001b[0;36mAcrobotEnv.step\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m    197\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m s \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCall reset before using AcrobotEnv object.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 199\u001b[0m torque \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAVAIL_TORQUE\u001b[49m\u001b[43m[\u001b[49m\u001b[43ma\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;66;03m# Add noise to the force action\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtorque_noise_max \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add the 'nets' folder to the system path to import the necessary modules\n",
    "sys.path.append('./nets')\n",
    "\n",
    "from DDPGAgent import DDPGAgent, DDPGAgentConfig\n",
    "from Memory import Memory, MemoryConfig\n",
    "\n",
    "# Utility functions\n",
    "def save_frames_as_gif(frames, path='./', filename='gym_animation.gif'):\n",
    "    import imageio\n",
    "    imageio.mimsave(os.path.join(path, filename), frames, fps=30)\n",
    "\n",
    "def plot_learning_curve(scores, filename):\n",
    "    x = [i+1 for i in range(len(scores))]\n",
    "    plt.plot(x, scores)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Score')\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "\n",
    "def train_agent(agent, env, num_games, scores_plot_file):\n",
    "    scores = []\n",
    "    best_avg_score = -np.inf\n",
    "    \n",
    "    for t in range(num_games):\n",
    "        state, _ = env.reset()\n",
    "        terminated, truncated = False, False\n",
    "        score = 0\n",
    "        step = 0\n",
    "        agent.noise.reset()\n",
    "        while not (terminated or truncated):\n",
    "            step += 1\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            terminal = terminated or truncated\n",
    "            agent.memory.store(state, action, reward, next_state, terminal)\n",
    "            agent.learn()\n",
    "            score += reward \n",
    "            state = next_state\n",
    "        scores.append(score)\n",
    "        \n",
    "        avg_score = np.mean(scores[-100:])\n",
    "        print(f\"game {t}, steps {step}, score {score:.2f}, avg_score {avg_score:.2f}\")\n",
    "        if avg_score > best_avg_score:\n",
    "            agent.save_models()\n",
    "            best_avg_score = avg_score\n",
    "\n",
    "    plot_learning_curve(scores, scores_plot_file)\n",
    "\n",
    "def test_agent(agent, env, final_landing_file):\n",
    "    agent.load_models()\n",
    "    \n",
    "    frames = []\n",
    "    terminated, truncated = False, False\n",
    "    score = 0\n",
    "    state, _ = env.reset()\n",
    "    step = 0\n",
    "    agent.noise.reset()\n",
    "    while not (terminated or truncated):\n",
    "        step += 1\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        terminal = terminated or truncated\n",
    "        score += reward \n",
    "        state = next_state\n",
    "        frames.append(env.render())\n",
    "\n",
    "    print(f\"score {score:.2f}\")      \n",
    "    save_frames_as_gif(frames, final_landing_file)            \n",
    "\n",
    "def main(mode=\"train\"):\n",
    "    env_name = \"Acrobot-v1\"\n",
    "    env = gym.make(env_name)\n",
    "    num_games = 1000\n",
    "    a_lr = 0.0001\n",
    "    c_lr = 0.001\n",
    "    gamma = 0.99\n",
    "    tau = 0.001\n",
    "    input_size = env.observation_space.shape[0]\n",
    "    fcl1_size = 400\n",
    "    fcl2_size = 300\n",
    "    actions_num = env.action_space.n  # Note the change to .n for discrete actions\n",
    "    memory_size = 1000000\n",
    "    batch_size = 64\n",
    "    \n",
    "    file_name = f\"DDPG_{env_name}_{a_lr}_{c_lr}_{num_games}\"\n",
    "    scores_plot_file = f\"./plots/{file_name}.png\"\n",
    "    final_landing_file = f\"./plots/{file_name}.gif\"\n",
    "    actor_file_name = f\"Actor_DDPG_{env_name}_{a_lr}_{num_games}\"\n",
    "    critic_file_name = f\"Critic_DDPG_{env_name}_{c_lr}_{num_games}\"\n",
    "    oa_mf = f\"./models/Online_{actor_file_name}.h5\"\n",
    "    oc_mf = f\"./models/Online_{critic_file_name}.h5\"\n",
    "    ta_mf = f\"./models/Target_{actor_file_name}.h5\"\n",
    "    tc_mf = f\"./models/Target_{critic_file_name}.h5\"\n",
    "    \n",
    "    agent_config = DDPGAgentConfig(\n",
    "        actor_lr=a_lr,\n",
    "        critic_lr=c_lr,\n",
    "        gamma=gamma,\n",
    "        tau=tau,\n",
    "        input_dim=input_size,\n",
    "        fc1_units=fcl1_size,\n",
    "        fc2_units=fcl2_size,\n",
    "        action_dim=actions_num,\n",
    "        memory_size=memory_size,\n",
    "        batch_size=batch_size,\n",
    "        actor_model_file=oa_mf,\n",
    "        critic_model_file=oc_mf,\n",
    "        target_actor_model_file=ta_mf,\n",
    "        target_critic_model_file=tc_mf\n",
    "    )\n",
    "    \n",
    "    agent = DDPGAgent(agent_config)\n",
    "    \n",
    "    if mode == \"train\":\n",
    "        train_agent(agent, env, num_games, scores_plot_file)\n",
    "    elif mode == \"test\":\n",
    "        test_agent(agent, env, final_landing_file)\n",
    "\n",
    "# Run the main function\n",
    "main(mode=\"train\")  # Change to \"test\" for testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-11 06:11:36.038474: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-11 06:11:36.047074: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-11 06:11:36.058576: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-11 06:11:36.058595: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-11 06:11:36.065493: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-11 06:11:36.887499: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/arash/.local/lib/python3.10/site-packages/pydantic/_internal/_fields.py:161: UserWarning: Field \"model_file\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/home/arash/.local/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "2024-07-11 06:11:38.147232: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-11 06:11:38.183040: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-11 06:11:38.183075: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-11 06:11:38.184904: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-11 06:11:38.184928: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-11 06:11:38.184937: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-11 06:11:38.275666: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-11 06:11:38.275710: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-11 06:11:38.275715: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2019] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-07-11 06:11:38.275732: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-11 06:11:38.275748: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5563 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
      "/home/arash/.local/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "states: (64, 6), actions: (64, 3), rewards: (64,), next_states: (64, 6), dones: (64,)\n",
      "critic_grads: [None, None, None, None, None, None, None, None, None, None, None, None]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No gradients provided for any variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 127\u001b[0m\n\u001b[1;32m    124\u001b[0m         test_agent(agent, env, final_landing_file)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# Run the main function\u001b[39;00m\n\u001b[0;32m--> 127\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Change to \"test\" for testing\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 122\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(mode)\u001b[0m\n\u001b[1;32m    119\u001b[0m agent \u001b[38;5;241m=\u001b[39m DDPGAgent(agent_config)\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 122\u001b[0m     \u001b[43mtrain_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_games\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscores_plot_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    124\u001b[0m     test_agent(agent, env, final_landing_file)\n",
      "Cell \u001b[0;32mIn[1], line 43\u001b[0m, in \u001b[0;36mtrain_agent\u001b[0;34m(agent, env, num_games, scores_plot_file)\u001b[0m\n\u001b[1;32m     41\u001b[0m terminal \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n\u001b[1;32m     42\u001b[0m agent\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39mstore(state, action, reward, next_state, terminal)\n\u001b[0;32m---> 43\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward \n\u001b[1;32m     45\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n",
      "File \u001b[0;32m~/Acrobat-Gym-RL/DDPG/./nets/DDPGAgent.py:198\u001b[0m, in \u001b[0;36mDDPGAgent.learn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    196\u001b[0m critic_grads \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(critic_loss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39monline_critic\u001b[38;5;241m.\u001b[39mtrainable_variables)\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcritic_grads: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcritic_grads\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 198\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43monline_critic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcritic_grads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43monline_critic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainable_variables\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[1;32m    201\u001b[0m     actions_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39monline_actor(states)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/optimizers/base_optimizer.py:282\u001b[0m, in \u001b[0;36mBaseOptimizer.apply_gradients\u001b[0;34m(self, grads_and_vars)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_gradients\u001b[39m(\u001b[38;5;28mself\u001b[39m, grads_and_vars):\n\u001b[1;32m    281\u001b[0m     grads, trainable_variables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mgrads_and_vars)\n\u001b[0;32m--> 282\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainable_variables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;66;03m# Return iterations for compat with tf.keras.\u001b[39;00m\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterations\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/optimizers/base_optimizer.py:335\u001b[0m, in \u001b[0;36mBaseOptimizer.apply\u001b[0;34m(self, grads, trainable_variables)\u001b[0m\n\u001b[1;32m    328\u001b[0m grads, trainable_variables \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_overwrite_variables_directly_with_gradients(\n\u001b[1;32m    330\u001b[0m         grads, trainable_variables\n\u001b[1;32m    331\u001b[0m     )\n\u001b[1;32m    332\u001b[0m )\n\u001b[1;32m    334\u001b[0m \u001b[38;5;66;03m# Filter empty gradients.\u001b[39;00m\n\u001b[0;32m--> 335\u001b[0m grads, trainable_variables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_filter_empty_gradients\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainable_variables\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mlist\u001b[39m(grads)) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/optimizers/base_optimizer.py:662\u001b[0m, in \u001b[0;36mBaseOptimizer._filter_empty_gradients\u001b[0;34m(self, grads, vars)\u001b[0m\n\u001b[1;32m    659\u001b[0m         missing_grad_vars\u001b[38;5;241m.\u001b[39mappend(v\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m    661\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m filtered_grads:\n\u001b[0;32m--> 662\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo gradients provided for any variable.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    663\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m missing_grad_vars:\n\u001b[1;32m    664\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    665\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGradients do not exist for variables \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    666\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mreversed\u001b[39m(missing_grad_vars))\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m when minimizing the loss.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    667\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m If using `model.compile()`, did you forget to provide a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    668\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`loss` argument?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    669\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: No gradients provided for any variable."
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add the 'nets' folder to the system path to import the necessary modules\n",
    "sys.path.append('./nets')\n",
    "\n",
    "from DDPGAgent import DDPGAgent, DDPGAgentConfig\n",
    "from Memory import Memory, MemoryConfig\n",
    "\n",
    "# Utility functions\n",
    "def save_frames_as_gif(frames, path='./', filename='gym_animation.gif'):\n",
    "    import imageio\n",
    "    imageio.mimsave(os.path.join(path, filename), frames, fps=30)\n",
    "\n",
    "def plot_learning_curve(scores, filename):\n",
    "    x = [i+1 for i in range(len(scores))]\n",
    "    plt.plot(x, scores)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Score')\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "\n",
    "def train_agent(agent, env, num_games, scores_plot_file):\n",
    "    scores = []\n",
    "    best_avg_score = -np.inf\n",
    "    \n",
    "    for t in range(num_games):\n",
    "        state, _ = env.reset()\n",
    "        terminated, truncated = False, False\n",
    "        score = 0\n",
    "        step = 0\n",
    "        agent.noise.reset()\n",
    "        while not (terminated or truncated):\n",
    "            step += 1\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            terminal = terminated or truncated\n",
    "            agent.memory.store(state, action, reward, next_state, terminal)\n",
    "            agent.learn()\n",
    "            score += reward \n",
    "            state = next_state\n",
    "        scores.append(score)\n",
    "        \n",
    "        avg_score = np.mean(scores[-100:])\n",
    "        print(f\"game {t}, steps {step}, score {score:.2f}, avg_score {avg_score:.2f}\")\n",
    "        if avg_score > best_avg_score:\n",
    "            agent.save_models()\n",
    "            best_avg_score = avg_score\n",
    "\n",
    "    plot_learning_curve(scores, scores_plot_file)\n",
    "\n",
    "def test_agent(agent, env, final_landing_file):\n",
    "    agent.load_models()\n",
    "    \n",
    "    frames = []\n",
    "    terminated, truncated = False, False\n",
    "    score = 0\n",
    "    state, _ = env.reset()\n",
    "    step = 0\n",
    "    agent.noise.reset()\n",
    "    while not (terminated or truncated):\n",
    "        step += 1\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        terminal = terminated or truncated\n",
    "        score += reward \n",
    "        state = next_state\n",
    "        frames.append(env.render())\n",
    "\n",
    "    print(f\"score {score:.2f}\")      \n",
    "    save_frames_as_gif(frames, final_landing_file)            \n",
    "\n",
    "def main(mode=\"train\"):\n",
    "    env_name = \"Acrobot-v1\"\n",
    "    env = gym.make(env_name)\n",
    "    num_games = 1000\n",
    "    a_lr = 0.0001\n",
    "    c_lr = 0.001\n",
    "    gamma = 0.99\n",
    "    tau = 0.001\n",
    "    input_size = env.observation_space.shape[0]\n",
    "    fcl1_size = 400\n",
    "    fcl2_size = 300\n",
    "    actions_num = env.action_space.n  # Note the change to .n for discrete actions\n",
    "    memory_size = 1000000\n",
    "    batch_size = 64\n",
    "    \n",
    "    file_name = f\"DDPG_{env_name}_{a_lr}_{c_lr}_{num_games}\"\n",
    "    scores_plot_file = f\"./plots/{file_name}.png\"\n",
    "    final_landing_file = f\"./plots/{file_name}.gif\"\n",
    "    actor_file_name = f\"Actor_DDPG_{env_name}_{a_lr}_{num_games}\"\n",
    "    critic_file_name = f\"Cric_DDPG_{env_name}_{c_lr}_{num_games}\"\n",
    "    oa_mf = f\"./models/Online_{actor_file_name}.h5\"\n",
    "    oc_mf = f\"./models/Online_{critic_file_name}.h5\"\n",
    "    ta_mf = f\"./models/Target_{actor_file_name}.h5\"\n",
    "    tc_mf = f\"./models/Target_{critic_file_name}.h5\"\n",
    "    \n",
    "    agent_config = DDPGAgentConfig(\n",
    "        actor_lr=a_lr,\n",
    "        critic_lr=c_lr,\n",
    "        gamma=gamma,\n",
    "        tau=tau,\n",
    "        input_dim=input_size,\n",
    "        fc1_units=fcl1_size,\n",
    "        fc2_units=fcl2_size,\n",
    "        action_dim=actions_num,\n",
    "        memory_size=memory_size,\n",
    "        batch_size=batch_size,\n",
    "        actor_model_file=oa_mf,\n",
    "        critic_model_file=oc_mf,\n",
    "        target_actor_model_file=ta_mf,\n",
    "        target_critic_model_file=tc_mf\n",
    "    )\n",
    "    \n",
    "    agent = DDPGAgent(agent_config)\n",
    "    \n",
    "    if mode == \"train\":\n",
    "        train_agent(agent, env, num_games, scores_plot_file)\n",
    "    elif mode == \"test\":\n",
    "        test_agent(agent, env, final_landing_file)\n",
    "\n",
    "# Run the main function\n",
    "main(mode=\"train\")  # Change to \"test\" for testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
