{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-11 05:46:15.561139: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-11 05:46:15.569861: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-11 05:46:15.580742: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-11 05:46:15.580763: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-11 05:46:15.587570: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-11 05:46:16.390789: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/arash/.local/lib/python3.10/site-packages/pydantic/_internal/_fields.py:161: UserWarning: Field \"model_file\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/home/arash/.local/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "2024-07-11 05:46:17.706469: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-11 05:46:17.744219: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-11 05:46:17.744250: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-11 05:46:17.746951: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-11 05:46:17.746994: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-11 05:46:17.747005: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-11 05:46:17.827455: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-11 05:46:17.827497: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-11 05:46:17.827502: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2019] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-07-11 05:46:17.827519: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-11 05:46:17.827536: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5563 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
      "/home/arash/.local/lib/python3.10/site-packages/keras/src/optimizers/base_optimizer.py:33: UserWarning: Argument `decay` is no longer supported and will be ignored.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add the 'nets' folder to the system path to import the necessary modules\n",
    "sys.path.append('./nets')\n",
    "\n",
    "from DDPGAgent import DDPGAgent, DDPGAgentConfig\n",
    "from Memory import Memory, MemoryConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_frames_as_gif(frames, path='./', filename='gym_animation.gif'):\n",
    "    import imageio\n",
    "    imageio.mimsave(os.path.join(path, filename), frames, fps=30)\n",
    "\n",
    "def plot_learning_curve(scores, filename):\n",
    "    x = [i+1 for i in range(len(scores))]\n",
    "    plt.plot(x, scores)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Score')\n",
    "    plt.savefig(filename)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(agent, env, num_games, scores_plot_file):\n",
    "    scores = []\n",
    "    best_avg_score = -np.inf\n",
    "    \n",
    "    for t in range(num_games):\n",
    "        state, _ = env.reset()\n",
    "        terminated, truncated = False, False\n",
    "        score = 0\n",
    "        step = 0\n",
    "        agent.noise.reset()\n",
    "        while not (terminated or truncated):\n",
    "            step += 1\n",
    "            action = agent.act(state)\n",
    "            action = np.argmax(action)  # Convert continuous action to discrete action\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            terminal = terminated or truncated\n",
    "            agent.memory.store(state, action, reward, next_state, terminal)\n",
    "            agent.learn()\n",
    "            score += reward \n",
    "            state = next_state\n",
    "        scores.append(score)\n",
    "        \n",
    "        avg_score = np.mean(scores[-100:])\n",
    "        print(f\"game {t}, steps {step}, score {score:.2f}, avg_score {avg_score:.2f}\")\n",
    "        if avg_score > best_avg_score:\n",
    "            agent.save_models()\n",
    "            best_avg_score = avg_score\n",
    "\n",
    "    plot_learning_curve(scores, scores_plot_file)\n",
    "\n",
    "def test_agent(agent, env, final_landing_file):\n",
    "    agent.load_models()\n",
    "    \n",
    "    frames = []\n",
    "    terminated, truncated = False, False\n",
    "    score = 0\n",
    "    state, _ = env.reset()\n",
    "    step = 0\n",
    "    agent.noise.reset()\n",
    "    while not (terminated or truncated):\n",
    "        step += 1\n",
    "        action = agent.act(state)\n",
    "        action = np.argmax(action)  # Convert continuous action to discrete action\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        terminal = terminated or truncated\n",
    "        score += reward \n",
    "        state = next_state\n",
    "        frames.append(env.render())\n",
    "\n",
    "    print(f\"score {score:.2f}\")      \n",
    "    save_frames_as_gif(frames, final_landing_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arash/.local/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "states: (64, 6), actions: (64, 3, 3), rewards: (64,), next_states: (64, 6), dones: (64,)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "missing a required argument: 'action'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 51\u001b[0m\n\u001b[1;32m     48\u001b[0m         test_agent(agent, env, final_landing_file)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Run the main function\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Change to \"test\" for testing\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 46\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(mode)\u001b[0m\n\u001b[1;32m     43\u001b[0m agent \u001b[38;5;241m=\u001b[39m DDPGAgent(agent_config)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 46\u001b[0m     \u001b[43mtrain_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_games\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscores_plot_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     48\u001b[0m     test_agent(agent, env, final_landing_file)\n",
      "Cell \u001b[0;32mIn[5], line 18\u001b[0m, in \u001b[0;36mtrain_agent\u001b[0;34m(agent, env, num_games, scores_plot_file)\u001b[0m\n\u001b[1;32m     16\u001b[0m terminal \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n\u001b[1;32m     17\u001b[0m agent\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39mstore(state, action, reward, next_state, terminal)\n\u001b[0;32m---> 18\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward \n\u001b[1;32m     20\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n",
      "File \u001b[0;32m~/Acrobat-Gym-RL/DDPG/./nets/DDPGAgent.py:186\u001b[0m, in \u001b[0;36mDDPGAgent.learn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstates: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstates\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, actions: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mactions\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, rewards: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrewards\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, next_states: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnext_states\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, dones: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdones\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    185\u001b[0m target_actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_actor(next_states)\n\u001b[0;32m--> 186\u001b[0m target_Q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_critic\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnext_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_actions\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m online_Q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39monline_critic([states, actions])\n\u001b[1;32m    189\u001b[0m target_Q_values \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mwhere(dones, tf\u001b[38;5;241m.\u001b[39mzeros_like(target_Q_values), target_Q_values)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/usr/lib/python3.10/inspect.py:3186\u001b[0m, in \u001b[0;36mSignature.bind\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3181\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m/\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   3182\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get a BoundArguments object, that maps the passed `args`\u001b[39;00m\n\u001b[1;32m   3183\u001b[0m \u001b[38;5;124;03m    and `kwargs` to the function's signature.  Raises `TypeError`\u001b[39;00m\n\u001b[1;32m   3184\u001b[0m \u001b[38;5;124;03m    if the passed arguments can not be bound.\u001b[39;00m\n\u001b[1;32m   3185\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bind\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/inspect.py:3101\u001b[0m, in \u001b[0;36mSignature._bind\u001b[0;34m(self, args, kwargs, partial)\u001b[0m\n\u001b[1;32m   3099\u001b[0m                 msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmissing a required argument: \u001b[39m\u001b[38;5;132;01m{arg!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   3100\u001b[0m                 msg \u001b[38;5;241m=\u001b[39m msg\u001b[38;5;241m.\u001b[39mformat(arg\u001b[38;5;241m=\u001b[39mparam\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m-> 3101\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3102\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3103\u001b[0m     \u001b[38;5;66;03m# We have a positional argument to process\u001b[39;00m\n\u001b[1;32m   3104\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: missing a required argument: 'action'"
     ]
    }
   ],
   "source": [
    "def main(mode=\"train\"):\n",
    "    env_name = \"Acrobot-v1\"\n",
    "    env = gym.make(env_name)\n",
    "    num_games = 1000\n",
    "    a_lr = 0.0001\n",
    "    c_lr = 0.001\n",
    "    gamma = 0.99\n",
    "    tau = 0.001\n",
    "    input_size = env.observation_space.shape[0]\n",
    "    fcl1_size = 400\n",
    "    fcl2_size = 300\n",
    "    actions_num = env.action_space.n  # Note the change to .n for discrete actions\n",
    "    memory_size = 1000000\n",
    "    batch_size = 64\n",
    "    \n",
    "    file_name = f\"DDPG_{env_name}_{a_lr}_{c_lr}_{num_games}\"\n",
    "    scores_plot_file = f\"./plots/{file_name}.png\"\n",
    "    final_landing_file = f\"./plots/{file_name}.gif\"\n",
    "    actor_file_name = f\"Actor_DDPG_{env_name}_{a_lr}_{num_games}\"\n",
    "    critic_file_name = f\"Critic_DDPG_{env_name}_{c_lr}_{num_games}\"\n",
    "    oa_mf = f\"./models/Online_{actor_file_name}.h5\"\n",
    "    oc_mf = f\"./models/Online_{critic_file_name}.h5\"\n",
    "    ta_mf = f\"./models/Target_{actor_file_name}.h5\"\n",
    "    tc_mf = f\"./models/Target_{critic_file_name}.h5\"\n",
    "    \n",
    "    agent_config = DDPGAgentConfig(\n",
    "        actor_lr=a_lr,\n",
    "        critic_lr=c_lr,\n",
    "        gamma=gamma,\n",
    "        tau=tau,\n",
    "        input_dim=input_size,\n",
    "        fc1_units=fcl1_size,\n",
    "        fc2_units=fcl2_size,\n",
    "        action_dim=actions_num,\n",
    "        memory_size=memory_size,\n",
    "        batch_size=batch_size,\n",
    "        actor_model_file=oa_mf,\n",
    "        critic_model_file=oc_mf,\n",
    "        target_actor_model_file=ta_mf,\n",
    "        target_critic_model_file=tc_mf\n",
    "    )\n",
    "    \n",
    "    agent = DDPGAgent(agent_config)\n",
    "    \n",
    "    if mode == \"train\":\n",
    "        train_agent(agent, env, num_games, scores_plot_file)\n",
    "    elif mode == \"test\":\n",
    "        test_agent(agent, env, final_landing_file)\n",
    "\n",
    "# Run the main function\n",
    "main(mode=\"train\")  # Change to \"test\" for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
